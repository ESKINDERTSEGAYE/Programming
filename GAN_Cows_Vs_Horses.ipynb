{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Cows_Vs_Horses.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNIjwnO+rN6kmEMUl2NP+Iz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ESKINDERTSEGAYE/Programming/blob/main/GAN_Cows_Vs_Horses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74e8bmXzodI7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras import layers, models, optimizers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocesssing"
      ],
      "metadata": {
        "id": "segQT0JL1mb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YsmuU0vewU6",
        "outputId": "84379d9f-1b55-4728-e29c-3aac813b206e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip '/content/drive/MyDrive/Colab Notebooks (1)/Cows_vs_Horses.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35sfLtXazK4i",
        "outputId": "800e4f75-c48c-4d2d-9608-5e62588b3f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks (1)/Cows_vs_Horses.zip\n",
            "   creating: Newdata/\n",
            "   creating: Newdata/Train/\n",
            "   creating: Newdata/Train/Cow/\n",
            "  inflating: Newdata/Train/Cow/cow3-000-000.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-022-000.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-022-090.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-022-180.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-022-270.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-035-045.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-035-135.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-035-225.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-035-315.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-045-000.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-045-090.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-045-180.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-045-270.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-066-027.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-066-063.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-066-117.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-066-153.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-066-207.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-066-243.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-066-297.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-066-333.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-068-000.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-068-090.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-068-180.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-068-270.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-000.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-022.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-045.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-068.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-090.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-112.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-135.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-158.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-180.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-202.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-225.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-248.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-270.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-292.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-315.png  \n",
            "  inflating: Newdata/Train/Cow/cow3-090-338.png  \n",
            "   creating: Newdata/Train/Horse/\n",
            "  inflating: Newdata/Train/Horse/horse5-000-000.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-022-000.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-022-090.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-022-180.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-022-270.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-035-045.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-035-135.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-035-225.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-035-315.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-045-000.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-045-090.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-045-180.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-045-270.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-066-027.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-066-063.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-066-117.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-066-153.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-066-207.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-066-243.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-066-297.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-066-333.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-068-000.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-068-090.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-068-180.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-068-270.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-000.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-022.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-045.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-068.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-090.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-112.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-135.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-158.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-180.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-202.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-225.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-248.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-270.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-292.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-315.png  \n",
            "  inflating: Newdata/Train/Horse/horse5-090-338.png  \n",
            "   creating: Newdata/Test/\n",
            "   creating: Newdata/Test/Shape1/\n",
            "  inflating: Newdata/Test/Shape1/cow8-000-000.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-022-000.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-022-090.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-022-180.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-022-270.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-035-045.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-035-135.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-035-225.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-035-315.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-045-000.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-045-090.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-045-180.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-045-270.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-066-027.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-066-063.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-066-117.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-066-153.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-066-207.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-066-243.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-066-297.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-066-333.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-068-000.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-068-090.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-068-180.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-068-270.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-000.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-022.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-045.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-068.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-090.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-112.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-135.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-158.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-180.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-202.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-225.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-248.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-270.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-292.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-315.png  \n",
            "  inflating: Newdata/Test/Shape1/cow8-090-338.png  \n",
            "   creating: Newdata/Test/Shape2/\n",
            "  inflating: Newdata/Test/Shape2/horse3-000-000.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-022-000.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-022-090.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-022-180.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-022-270.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-035-045.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-035-135.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-035-225.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-035-315.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-045-000.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-045-090.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-045-180.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-045-270.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-066-027.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-066-063.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-066-117.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-066-153.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-066-207.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-066-243.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-066-297.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-066-333.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-068-000.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-068-090.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-068-180.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-068-270.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-000.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-022.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-045.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-068.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-090.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-112.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-135.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-158.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-180.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-202.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-225.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-248.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-270.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-292.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-315.png  \n",
            "  inflating: Newdata/Test/Shape2/horse3-090-338.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=5,\n",
        "    zoom_range=(0.95, 0.95),\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    data_format=\"channels_last\",\n",
        "    validation_split=0.0,\n",
        "    dtype=tf.float32,)\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255)"
      ],
      "metadata": {
        "id": "3_saEBwc_gPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_width = 256\n",
        "image_height = 256"
      ],
      "metadata": {
        "id": "lw94X4AJ3wcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Processing\n",
        "dataset_train = train_datagen.flow_from_directory(\"Newdata/Train\", shuffle=True, batch_size=16, target_size=(image_width, image_height), class_mode='binary')\n",
        "dataset_test = val_datagen.flow_from_directory(\"Newdata/Test\", shuffle=False, batch_size=16, target_size=(image_width, image_height), class_mode='binary')"
      ],
      "metadata": {
        "id": "fnmRnj6UCnyU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcb4bdb-301a-4ce5-8aec-b038f9b901eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 82 images belonging to 2 classes.\n",
            "Found 82 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "pSTE2N2s1rmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), input_shape=(image_width, image_height, 3)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1))\n",
        "model.add(layers.Activation('sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrS-Mxpt1tvk",
        "outputId": "9b22d83c-25cb-4c9d-efa9-77859bf6b3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 254, 254, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 125, 125, 32)      9248      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 125, 125, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 62, 62, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 60, 60, 64)        18496     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 60, 60, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 57600)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                3686464   \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 64)                0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,715,169\n",
            "Trainable params: 3,715,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset_train, epochs=15, validation_data=dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrcGw0Q3HxeV",
        "outputId": "79d8ea23-b834-4931-ace0-66699fb151f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "6/6 [==============================] - 14s 318ms/step - loss: 0.9625 - accuracy: 0.5122 - val_loss: 0.7449 - val_accuracy: 0.5000\n",
            "Epoch 2/15\n",
            "6/6 [==============================] - 2s 266ms/step - loss: 0.7517 - accuracy: 0.6463 - val_loss: 0.9883 - val_accuracy: 0.5000\n",
            "Epoch 3/15\n",
            "6/6 [==============================] - 2s 270ms/step - loss: 0.6429 - accuracy: 0.6220 - val_loss: 0.6937 - val_accuracy: 0.6341\n",
            "Epoch 4/15\n",
            "6/6 [==============================] - 2s 263ms/step - loss: 0.6996 - accuracy: 0.6463 - val_loss: 0.8080 - val_accuracy: 0.5000\n",
            "Epoch 5/15\n",
            "6/6 [==============================] - 2s 313ms/step - loss: 0.5105 - accuracy: 0.7805 - val_loss: 1.6204 - val_accuracy: 0.5000\n",
            "Epoch 6/15\n",
            "6/6 [==============================] - 2s 264ms/step - loss: 0.3925 - accuracy: 0.7805 - val_loss: 0.7101 - val_accuracy: 0.6220\n",
            "Epoch 7/15\n",
            "6/6 [==============================] - 2s 265ms/step - loss: 0.1096 - accuracy: 0.9756 - val_loss: 2.6586 - val_accuracy: 0.5000\n",
            "Epoch 8/15\n",
            "6/6 [==============================] - 2s 263ms/step - loss: 0.2819 - accuracy: 0.8902 - val_loss: 1.3712 - val_accuracy: 0.5732\n",
            "Epoch 9/15\n",
            "6/6 [==============================] - 2s 267ms/step - loss: 0.6818 - accuracy: 0.8415 - val_loss: 0.9009 - val_accuracy: 0.5976\n",
            "Epoch 10/15\n",
            "6/6 [==============================] - 2s 265ms/step - loss: 0.1221 - accuracy: 0.9634 - val_loss: 1.0848 - val_accuracy: 0.6098\n",
            "Epoch 11/15\n",
            "6/6 [==============================] - 2s 262ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 2.0621 - val_accuracy: 0.5732\n",
            "Epoch 12/15\n",
            "6/6 [==============================] - 2s 261ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 2.4311 - val_accuracy: 0.5732\n",
            "Epoch 13/15\n",
            "6/6 [==============================] - 2s 262ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 2.4639 - val_accuracy: 0.5976\n",
            "Epoch 14/15\n",
            "6/6 [==============================] - 2s 270ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 2.8377 - val_accuracy: 0.5854\n",
            "Epoch 15/15\n",
            "6/6 [==============================] - 2s 259ms/step - loss: 0.0894 - accuracy: 0.9756 - val_loss: 2.9852 - val_accuracy: 0.5732\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f577cc7c6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model in Keras"
      ],
      "metadata": {
        "id": "Zvu2-e00KwEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('model_wieghts.h5')\n",
        "model.save('model_keras.h5')"
      ],
      "metadata": {
        "id": "0D9ql42YKreK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN PART"
      ],
      "metadata": {
        "id": "xhbglfZQURfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Reshape, Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "\n"
      ],
      "metadata": {
        "id": "8JVwdWW68dnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the discriminator model\n",
        "def define_D(in_shape=(128,128,3)):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ncavxkakd1Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the generator model\n",
        "def define_G(latent_dim):\n",
        "    model = Sequential()\n",
        "\t  # foundation for 16x16 image\n",
        "    n_nodes = 256 * 16 * 16\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((16, 16, 256)))\n",
        "    # upsample to 32x32\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # upsample to 64x64\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    # upsamplde to 128x128\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(3, (7,7), activation='tanh', padding='same'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "3R7-ICN5d-5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_GAN(model_G, model_D):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\tmodel_D.trainable = False\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(model_G)\n",
        "\tmodel.add(model_D)\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "NQWGPypXeDhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_real_images():\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "    X = datagen.flow_from_directory('Newdata/Train',\n",
        "                                    target_size= (128,128),\n",
        "                                    batch_size=12500,\n",
        "                                    class_mode='binary')\n",
        "    data_list = []\n",
        "    batch_index = 0\n",
        "    while batch_index <= X.batch_index:\n",
        "        data = X.next()\n",
        "        data_list.append(data[0])\n",
        "        batch_index += 1\n",
        "    img_array = np.asarray(data_list)\n",
        "    return img_array"
      ],
      "metadata": {
        "id": "K6o2HVsBgeZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_images(dataset, n_samples):\n",
        "    i = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "    X = dataset[i]\n",
        "    y = np.ones((n_samples,1))\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "LmhfpccKgz9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    X = np.random.randn(latent_dim * n_samples)\n",
        "    X = X.reshape(n_samples, latent_dim)\n",
        "    return X"
      ],
      "metadata": {
        "id": "HStNZZ6rg3r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_images(model_G, latent_dim, n_samples):\n",
        "    X_input = generate_latent_points(latent_dim, n_samples)\n",
        "    X = model_G.predict(X_input)\n",
        "    y = np.zeros((n_samples, 1))\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "l0kw85hIg7-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_performance(epoch, model_G, model_D, dataset, latent_dim, n_samples=100):\n",
        "  model_G.save('newdata_' +str(epoch)+ '.h5')\n",
        "  X_real, y_real = generate_real_images(dataset, n_samples)\n",
        "  _, acc_real = model_D.evaluate(X_real, y_real, verbose=0)\n",
        "  x_fake, y_fake = generate_fake_images(model_G, latent_dim, n_samples)\n",
        "  _, acc_fake = model_D.evaluate(x_fake, y_fake, verbose=0)\n",
        "  print('Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))"
      ],
      "metadata": {
        "id": "_plBrEuGhJUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_discriminator(model, dataset, n_iter=100, n_batch=82):\n",
        "    half_batch = int(n_batch/2)\n",
        "    # manually enumerate epochs\n",
        "    for i in range(n_iter):\n",
        "        X_real, y_real = generate_real_images(dataset, half_batch)\n",
        "        _, real_acc = model.train_on_batch(X_real, y_real)\n",
        "        X_fake, y_fake = generate_fake_images(half_batch)\n",
        "        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
        "        print('%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))"
      ],
      "metadata": {
        "id": "g3IrNHIVivs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_GAN(model_G, model_D, model_GAN, dataset, latent_dim, n_epochs=2000, n_batch=82):\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\tX_real, y_real = generate_real_images(dataset, half_batch)\n",
        "\t\t\tX_fake, y_fake = generate_fake_images(model_G, latent_dim, half_batch)\n",
        "\t\t\tX, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
        "\t\t\td_loss, _ = model_D.train_on_batch(X, y)\n",
        "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\ty_gan = np.ones((n_batch, 1))\n",
        "\t\t\tg_loss = model_GAN.train_on_batch(X_gan, y_gan)\n",
        "\t\t\tprint('%d, %d/%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
        "\t\t# evaluate the model performance\n",
        "\t\tif (i+1) % 10 == 0:\n",
        "\t\t\tsummarize_performance(i, model_G, model_D, dataset, latent_dim)"
      ],
      "metadata": {
        "id": "4yuWC_6kiyts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim =100\n",
        "model_D = define_D()\n",
        "model_G = define_G(latent_dim)\n",
        "model_GAN = define_GAN(model_G, model_D)\n",
        "dataset=load_real_images()\n",
        "train_GAN(model_G, model_D, model_GAN, dataset[0], latent_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rMdp5t9i7E_",
        "outputId": "16456401-bcef-4ebb-f08e-dd300eee854f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 82 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = np.random.randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn z_input"
      ],
      "metadata": {
        "id": "9Djn2wYcHT0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images, n):\n",
        "\t# scales image values in the range of [0,1]\n",
        "\timages = (images-images.min())/(images.max() - images.min())\n",
        "\tfor i in range(n):\n",
        "\t\t# define subplot\n",
        "\t\tplt.subplot(1, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tplt.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tplt.imshow(images[i, :, :])\n",
        "\tplt.show()"
      ],
      "metadata": {
        "id": "uISn44ZbjPMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pts = generate_latent_points(100, 50)\n",
        "# generate images\n",
        "X = model_G.predict(pts)\n",
        "# plot the result\n",
        "plot_images(X, 9)"
      ],
      "metadata": {
        "id": "DxdC46WqjSor",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "cb34faf0-a87a-4849-d6ad-b8f34a89997f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d7bb7a698bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_latent_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# generate images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plot the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_latent_points' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CycleGAN\n"
      ],
      "metadata": {
        "id": "N3N6J7-gGRNZ"
      }
    }
  ]
}